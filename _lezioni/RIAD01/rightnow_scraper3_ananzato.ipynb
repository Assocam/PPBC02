{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subito scraper\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avanzato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'replace'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 93\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m search_query[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_page\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m search_query[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_page\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m curr_page:\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 93\u001b[0m         tot_ads, items \u001b[38;5;241m=\u001b[39m \u001b[43mscrape_page\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurr_page\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mRequestException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m     95\u001b[0m         \u001b[38;5;28mprint\u001b[39m(err)\n",
      "Cell \u001b[0;32mIn[1], line 76\u001b[0m, in \u001b[0;36mscrape_page\u001b[0;34m(page)\u001b[0m\n\u001b[1;32m     70\u001b[0m             data[key] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     72\u001b[0m         \u001b[38;5;66;03m# data[key] = val[0].find(text=True, recursive=False) if val else None\u001b[39;00m\n\u001b[1;32m     73\u001b[0m         \u001b[38;5;66;03m# data[key] = val[0].contents[0] if val else None\u001b[39;00m\n\u001b[1;32m     75\u001b[0m     data\u001b[38;5;241m.\u001b[39mupdate({\n\u001b[0;32m---> 76\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprice\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprice\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplace\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\xa0\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSpedizione disponibile\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m     77\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m'\u001b[39m: product\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma.link\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m'\u001b[39m],  \n\u001b[1;32m     78\u001b[0m     })\n\u001b[1;32m     80\u001b[0m     items\u001b[38;5;241m.\u001b[39mappend(data)\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tot_ads, items\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'replace'"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from time import sleep\n",
    "from random import randint\n",
    "from collections import defaultdict\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "online = False\n",
    "\n",
    "search_url = 'https://www.subito.it/annunci-italia/vendita/usato/'\n",
    "\n",
    "# NOTA: Se non è indicato, è \"Dal più recente\"\n",
    "order_opts = ['priceasc', 'pricedesc', 'relevance']\n",
    "\n",
    "# Impostazioni di ricerca per lo scraper\n",
    "search_query = {\n",
    "    'search': 'sinclair+spectrum',\n",
    "    'order': 'relevance',\n",
    "    'max_page': None\n",
    "}\n",
    "\n",
    "if online or not 'res_cache' in globals():\n",
    "    # res_cache = defaultdict(type(None))\n",
    "    res_cache = defaultdict(lambda: None)\n",
    "\n",
    "# Funzione di estrazione articoli su una singola pagina della ricerca\n",
    "def scrape_page(page):\n",
    "    ## Send Request\n",
    "    query_str = {\n",
    "        'q': search_query['search'],\n",
    "        'o': page,\n",
    "        'order': search_query['order']\n",
    "    }\n",
    "\n",
    "    response = requests.get(search_url, query_str)\n",
    "\n",
    "    # Controlla che il server abbia risposto positivamente\n",
    "    if not response.status_code == 200:\n",
    "        raise requests.RequestException(f'Pagina {response.url} non trovata!')\n",
    " \n",
    "    ## Parse page\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Controlla che non sia la pagina \"Nessun risultato al momento...\"\n",
    "    no_results = soup.select('div[class*=ErrorLayout_container__]')\n",
    "    if no_results:\n",
    "        raise requests.RequestException('Non ci sono più pagine da analizzare.') \n",
    "\n",
    "    # Rilegge il numero di risultati totali (@todo)\n",
    "    tot_ads = soup.select('p[class*=AdsCount_total-ads__]')[0].get_text()\n",
    "\n",
    "    products = soup.select('div.items__item.item-card')\n",
    "    items = []\n",
    "\n",
    "    for product in products:\n",
    "        data = {\n",
    "            'name': product.select('h2[class*=ItemTitle-module_item-title__]'),\n",
    "            'town': product.select('span[class*=index-module_town__]'),\n",
    "            'city': product.select('span[class*=index-module_token-caption__].city'),\n",
    "            'date': product.select('span[class*=index-module_date__]'),\n",
    "            'price': product.select('p[class*=index-module_price__].price'),\n",
    "            'shipping': product.select('span.shipping-badge'),\n",
    "            'sold': product.select('span.item-sold-badge'),\n",
    "        }\n",
    "\n",
    "        for key, val in data.items():\n",
    "            if val:\n",
    "                data[key] = val[0].get_text()\n",
    "            else:\n",
    "                data[key] = None\n",
    "\n",
    "            # data[key] = val[0].find(text=True, recursive=False) if val else None\n",
    "            # data[key] = val[0].contents[0] if val else None\n",
    "\n",
    "        data.update({\n",
    "            'price': data['price'].replace('\\xa0', ' ').replace('Spedizione disponibile', ''),\n",
    "            'url': product.select('a.link')[0]['href'],  \n",
    "        })\n",
    "\n",
    "        items.append(data)\n",
    "    \n",
    "    return tot_ads, items\n",
    "    \n",
    "\n",
    "# Contatore per le pagine\n",
    "curr_page = 1\n",
    "# Contenitore per i dati estratti\n",
    "scraped_data = []\n",
    "while True:\n",
    "    # Se la max_page non è impostata oppure non si è ancora raggiunta la max_page\n",
    "    if not search_query['max_page'] or search_query['max_page'] >= curr_page:\n",
    "        try:\n",
    "            tot_ads, items = scrape_page(curr_page)\n",
    "        except requests.RequestException as err:\n",
    "            print(err)\n",
    "            break\n",
    "        \n",
    "        scraped_data += items\n",
    "        curr_page += 1\n",
    "        sleep(randint(1, 4))\n",
    "    # (Se la max_page è impostata e si è raggiunta)\n",
    "    else:\n",
    "        break\n",
    "\n",
    "print(tot_ads)\n",
    "pprint(scraped_data)\n",
    "\n",
    "full_data = {\n",
    "    'search': '',\n",
    "    'order': '',\n",
    "    # 'search_url': '',\n",
    "    'tot_ads': '',\n",
    "    'max_page': '',\n",
    "    'timestamp': '',\n",
    "    'items': '',\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
